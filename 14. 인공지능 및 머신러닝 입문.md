# 14. ì¸ê³µì§€ëŠ¥ ë° ë¨¸ì‹ ëŸ¬ë‹ ì…ë¬¸

## `scikit-learn` ì‚¬ìš©ë²•

### 1. ğŸ“¦ scikit-learnì´ë€?

`scikit-learn`ì€ **Python ê¸°ë°˜ì˜ ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬**ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ ì‘ì—…ì„ ë¹ ë¥´ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆê²Œ í•´ì¤˜:

- ë¶„ë¥˜(Classification)
- íšŒê·€(Regression)
- êµ°ì§‘(Clustering)
- ì°¨ì› ì¶•ì†Œ(Dimensionality Reduction)
- ëª¨ë¸ ì„ íƒ(Model Selection)
- ë°ì´í„° ì „ì²˜ë¦¬(Preprocessing)

> NumPy, SciPy, matplotlib ìœ„ì— êµ¬ì¶•ë˜ì–´ ìˆì–´. ëŒ€ê·œëª¨ ë”¥ëŸ¬ë‹ì´ ì•„ë‹ˆë¼ **ê¸°ì´ˆ í†µê³„/ML ì…ë¬¸ì— ìµœì í™”ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬**ì•¼.

### 2. ğŸ“¦ ì„¤ì¹˜ ë°©ë²•

```
pip install scikit-learn
```

ì¶”ê°€ì ìœ¼ë¡œ ë‹¤ìŒë„ ì„¤ì¹˜ ì¶”ì²œ:

```
pip install numpy pandas matplotlib seaborn
```

### 3. ğŸ§­ ì „ì²´ ì‚¬ìš© íë¦„ (ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸)

```
[ë°ì´í„° ìˆ˜ì§‘] â†’ [ì „ì²˜ë¦¬] â†’ [ëª¨ë¸ ì„ íƒ] â†’ [í•™ìŠµ (fit)] â†’ [ì˜ˆì¸¡ (predict)] â†’ [í‰ê°€ (score)]
```

`scikit-learn`ì€ ì´ ì „ì²´ íë¦„ì„ ì¼ê´€ëœ APIë¡œ ì§€ì›í•¨.

### 4. ğŸ“Œ ì£¼ìš” ê¸°ëŠ¥ë³„ í•µì‹¬ ì‚¬ìš©ë²•

#### âœ… 4.1 ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ì „ì²˜ë¦¬

```
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# ë°ì´í„° ë¡œë”©
iris = load_iris()
X, y = iris.data, iris.target

# í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ì •ê·œí™” (í‰ê· =0, ë¶„ì‚°=1)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
```

#### âœ… 4.2 ëª¨ë¸ ì„ íƒ ë° í•™ìŠµ

```
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
```

#### âœ… 4.3 ì˜ˆì¸¡ ë° í‰ê°€

```
from sklearn.metrics import accuracy_score, classification_report

y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))
```

### 5. ğŸ“Š ì£¼ìš” ë¶„ë¥˜ ëª¨ë¸ë“¤

| ëª¨ë¸                   | í´ë˜ìŠ¤ ì´ë¦„                   |
| ---------------------- | ----------------------------- |
| ë¡œì§€ìŠ¤í‹± íšŒê·€          | `LogisticRegression`          |
| ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹  (SVM) | `SVC`, `LinearSVC`            |
| ê²°ì • íŠ¸ë¦¬              | `DecisionTreeClassifier`      |
| ëœë¤ í¬ë ˆìŠ¤íŠ¸          | `RandomForestClassifier`      |
| K-ìµœê·¼ì ‘ ì´ì›ƒ          | `KNeighborsClassifier`        |
| ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ          | `GaussianNB`, `MultinomialNB` |
| ì‹ ê²½ë§                 | `MLPClassifier`               |

### 6. ğŸ“ˆ íšŒê·€ ëª¨ë¸ë“¤

| ëª¨ë¸               | í´ë˜ìŠ¤ ì´ë¦„             |
| ------------------ | ----------------------- |
| ì„ í˜• íšŒê·€          | `LinearRegression`      |
| ë¦¿ì§€ íšŒê·€          | `Ridge`                 |
| ë¼ì˜ íšŒê·€          | `Lasso`                 |
| ê²°ì • íŠ¸ë¦¬ íšŒê·€     | `DecisionTreeRegressor` |
| ëœë¤ í¬ë ˆìŠ¤íŠ¸ íšŒê·€ | `RandomForestRegressor` |
| ì„œí¬íŠ¸ ë²¡í„° íšŒê·€   | `SVR`                   |

### 7. ğŸ” êµì°¨ ê²€ì¦ ë° ëª¨ë¸ í‰ê°€

```
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5)
print("Cross-validation í‰ê·  ì •í™•ë„:", scores.mean())
```

### 8. ğŸ”„ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

```
from sklearn.model_selection import GridSearchCV

params = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 10, 20]
}
grid = GridSearchCV(RandomForestClassifier(), params, cv=5)
grid.fit(X_train, y_train)

print("ìµœì  íŒŒë¼ë¯¸í„°:", grid.best_params_)
```

### 9. ğŸ“‰ ì°¨ì› ì¶•ì†Œ (PCA ì˜ˆì‹œ)

```
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)

plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y)
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("PCA Visualization")
plt.show()
```

### ğŸ”š 10. ìš”ì•½

| ë‹¨ê³„   | í•¨ìˆ˜ / í´ë˜ìŠ¤                             | ì„¤ëª…                  |
| ------ | ----------------------------------------- | --------------------- |
| ë°ì´í„° | `datasets.load_*`, `train_test_split`     | ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°, ë¶„í•  |
| ì „ì²˜ë¦¬ | `StandardScaler`, `MinMaxScaler`          | ì •ê·œí™”, ìŠ¤ì¼€ì¼ë§      |
| ëª¨ë¸   | `RandomForestClassifier`, `SVC`, ...      | ëª¨ë¸ ì„ íƒ             |
| í•™ìŠµ   | `model.fit()`                             | í•™ìŠµ                  |
| ì˜ˆì¸¡   | `model.predict()`                         | ì˜ˆì¸¡                  |
| í‰ê°€   | `accuracy_score`, `classification_report` | ëª¨ë¸ ì„±ëŠ¥ í‰ê°€        |
| ê²€ì¦   | `cross_val_score`, `GridSearchCV`         | ëª¨ë¸ ì„ íƒê³¼ íŠœë‹      |

## ë°ì´í„° ì „ì²˜ë¦¬ ë° í•™ìŠµ

### 1. ğŸ§± ì „ì²˜ë¦¬ê°€ ì¤‘ìš”í•œ ì´ìœ 

ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì€ **ì“°ë ˆê¸° ë°ì´í„°ë¥¼ ë„£ìœ¼ë©´ ì“°ë ˆê¸° ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤**ëŠ” "GIGO(Garbage In, Garbage Out)" ì›ì¹™ì— ì² ì €íˆ ë”°ë¦„.

ë°ì´í„° ì „ì²˜ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œë¥¼ í•´ê²°í•´:

- ëˆ„ë½ëœ ê°’ (Missing Value)
- ë²”ì£¼í˜• ì²˜ë¦¬ (Categorical Encoding)
- ìŠ¤ì¼€ì¼ë§ ë° ì •ê·œí™”
- ì´ìƒì¹˜ ì²˜ë¦¬
- í´ë˜ìŠ¤ ë¶ˆê· í˜•
- ë°ì´í„° ë¶„í•  (Train/Test)

### 2. ğŸ§¼ ì£¼ìš” ì „ì²˜ë¦¬ ê¸°ë²• ì •ë¦¬

#### âœ… 2.1 ê²°ì¸¡ê°’ ì²˜ë¦¬

```
import numpy as np
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='mean')  # or median, most_frequent
X_filled = imputer.fit_transform(X)
```

#### âœ… 2.2 ë²”ì£¼í˜• ë°ì´í„° ì¸ì½”ë”©

```
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(sparse=False)
X_encoded = encoder.fit_transform(X_categorical)
```

ë˜ëŠ” `pandas.get_dummies()`ë¥¼ ì‚¬ìš©:

```
import pandas as pd
df = pd.get_dummies(df, columns=['sex', 'region'])
```

#### âœ… 2.3 íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ (Normalization / Standardization)

```
from sklearn.preprocessing import StandardScaler, MinMaxScaler

scaler = StandardScaler()  # í‰ê· =0, ë¶„ì‚°=1
X_scaled = scaler.fit_transform(X)

scaler = MinMaxScaler()    # 0~1 ì‚¬ì´ë¡œ ë³€í™˜
X_scaled = scaler.fit_transform(X)
```

#### âœ… 2.4 ì´ìƒì¹˜ ì œê±°

```
from scipy import stats
z_scores = np.abs(stats.zscore(X))
X_clean = X[(z_scores < 3).all(axis=1)]  # Z-score ê¸°ì¤€
```

ë˜ëŠ” IQR ë°©ì‹:

```
Q1 = np.percentile(X, 25, axis=0)
Q3 = np.percentile(X, 75, axis=0)
IQR = Q3 - Q1
mask = (X >= (Q1 - 1.5 * IQR)) & (X <= (Q3 + 1.5 * IQR))
X_clean = X[mask.all(axis=1)]
```

#### âœ… 2.5 í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ (ì˜¤ë²„ìƒ˜í”Œë§/ì–¸ë”ìƒ˜í”Œë§)

```
from imblearn.over_sampling import SMOTE
smote = SMOTE()
X_resampled, y_resampled = smote.fit_resample(X, y)
```

â€» `imblearn`ì€ ë³„ë„ ì„¤ì¹˜ í•„ìš”:

```
pip install imbalanced-learn
```

### 3. ğŸ§ª ë°ì´í„° ë¶„í• 

```
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
```

> `stratify=y`ë¥¼ ê¼­ ì¨ì¤˜ì•¼ í´ë˜ìŠ¤ ë¶„í¬ë¥¼ ìœ ì§€í•œ ìƒíƒœë¡œ ë¶„í• ë¼.

### 4. ğŸ” í•™ìŠµ (fit) ê³¼ì •

#### ê¸°ë³¸ íë¦„

```
model = SomeClassifier()
model.fit(X_train, y_train)
```

#### ì˜ˆ: ëœë¤ í¬ë ˆìŠ¤íŠ¸ í•™ìŠµ

```
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100, max_depth=5)
model.fit(X_train, y_train)
```

### 5. ğŸ”„ ì „ì²˜ë¦¬ì™€ í•™ìŠµì˜ ì—°ê²° â€” `Pipeline`

ëª¨ë“  ì „ì²˜ë¦¬ì™€ ëª¨ë¸ í•™ìŠµ ê³¼ì •ì„ í•˜ë‚˜ë¡œ ë¬¶ê¸°!

```
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('clf', LogisticRegression())
])

pipe.fit(X_train, y_train)
```

> ì´ì œ `pipe.predict()`ë§Œ ì“°ë©´ ëª¨ë“  ì „ì²˜ë¦¬ë¥¼ ìë™ ìˆ˜í–‰í•œ ë’¤ ì˜ˆì¸¡ê¹Œì§€ í•´ì¤Œ.

### 6. ğŸ“‰ í›ˆë ¨ ì •í™•ë„ vs í…ŒìŠ¤íŠ¸ ì •í™•ë„

| ìƒí™©           | ì˜ë¯¸                    | í•´ê²°ì±…                         |
| -------------- | ----------------------- | ------------------------------ |
| í›ˆë ¨â†‘, í…ŒìŠ¤íŠ¸â†“ | ê³¼ì í•© (overfitting)    | ëª¨ë¸ ë‹¨ìˆœí™”, ì •ê·œí™”, Dropout   |
| í›ˆë ¨â†“, í…ŒìŠ¤íŠ¸â†“ | ê³¼ì†Œì í•© (underfitting) | ëª¨ë¸ ë³µì¡ë„ ì¦ê°€, feature ì¶”ê°€ |
| í›ˆë ¨â‰ˆí…ŒìŠ¤íŠ¸    | ì¼ë°˜í™” ì–‘í˜¸             | Best ìƒíƒœ                      |

### 7. ğŸ“‹ ì „ì²˜ë¦¬ ìë™í™” ì˜ˆì‹œ ì½”ë“œ ì „ì²´

```
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# 1. ë°ì´í„°
data = load_breast_cancer()
X, y = data.data, data.target

# 2. ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)

# 3. íŒŒì´í”„ë¼ì¸ êµ¬ì„±
pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('clf', RandomForestClassifier(n_estimators=100))
])

# 4. í•™ìŠµ
pipe.fit(X_train, y_train)

# 5. í‰ê°€
y_pred = pipe.predict(X_test)
print("ì •í™•ë„:", accuracy_score(y_test, y_pred))
```

### âœ… ë§ˆë¬´ë¦¬ ìš”ì•½

| í•­ëª©          | ì„¤ëª…                             |
| ------------- | -------------------------------- |
| ê²°ì¸¡ê°’ ì²˜ë¦¬   | í‰ê· /ì¤‘ì•™ê°’ ë“±ìœ¼ë¡œ ì±„ì›€          |
| ë²”ì£¼í˜• ì¸ì½”ë”© | OneHotEncoder ë˜ëŠ” get_dummies   |
| ìŠ¤ì¼€ì¼ë§      | StandardScaler / MinMaxScaler    |
| ì´ìƒì¹˜ ì œê±°   | Z-score, IQR ë°©ë²•                |
| í´ë˜ìŠ¤ ë¶ˆê· í˜• | SMOTE ë“± ì˜¤ë²„ìƒ˜í”Œë§              |
| ë°ì´í„° ë¶„í•    | `train_test_split` ì‚¬ìš©          |
| íŒŒì´í”„ë¼ì¸    | ì „ì²˜ë¦¬ + ëª¨ë¸ í•™ìŠµì„ í•˜ë‚˜ë¡œ í†µí•© |
| ëª¨ë¸ í•™ìŠµ     | `fit()` ë©”ì„œë“œ ì‚¬ìš©              |

## ê¸°ë³¸ ëª¨ë¸ ì ìš© ë° í‰ê°€

### ğŸ§­ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ìš”ì•½

```
[ì „ì²˜ë¦¬] â†’ [ëª¨ë¸ ì„ íƒ] â†’ [í•™ìŠµ] â†’ [ì˜ˆì¸¡] â†’ [í‰ê°€]
```

ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ê²°ê³¼ë¥¼ ë¶„ì„í•˜ëŠ” ê²Œ í•µì‹¬ì¸ë°, ì–´ë–¤ ë¬¸ì œë¥¼ ë‹¤ë£¨ëŠ”ì§€ì— ë”°ë¼ í‰ê°€ ë°©ì‹ì´ ë‹¬ë¼ì ¸.

- **ë¶„ë¥˜ ë¬¸ì œ (Classification)**: ì´ì§„/ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜
- **íšŒê·€ ë¬¸ì œ (Regression)**: ì—°ì†ì ì¸ ìˆ˜ì¹˜ ì˜ˆì¸¡

### âœ… 1. ë¶„ë¥˜ ë¬¸ì œ: ì˜ˆì¸¡ ë° í‰ê°€

#### ğŸ“Œ ì˜ˆì¸¡

```
y_pred = model.predict(X_test)
```

í™•ë¥ ë¡œ ì˜ˆì¸¡í•˜ê³  ì‹¶ë‹¤ë©´:

```
y_prob = model.predict_proba(X_test)  # ê° í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥ 
```

#### ğŸ“Œ í‰ê°€ ì§€í‘œ

**ğŸ¯ 1) ì •í™•ë„ (Accuracy)**

```
from sklearn.metrics import accuracy_score
accuracy_score(y_test, y_pred)
```

> ì „ì²´ ì¤‘ ë§ì¶˜ ë¹„ìœ¨. **í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ì‹¬í•˜ë©´ ì™œê³¡ë  ìˆ˜ ìˆìŒ**

**ğŸŸ¨ 2) ì •ë°€ë„ / ì¬í˜„ìœ¨ / F1 ì ìˆ˜**

```
from sklearn.metrics import precision_score, recall_score, f1_score

precision_score(y_test, y_pred)
recall_score(y_test, y_pred)
f1_score(y_test, y_pred)
```

| ì§€í‘œ               | ì˜ë¯¸                               |
| ------------------ | ---------------------------------- |
| ì •ë°€ë„ (Precision) | ì˜ˆì¸¡ì´ ì°¸ì¸ ê²ƒ ì¤‘ ì‹¤ì œë¡œ ì°¸ì¸ ë¹„ìœ¨ |
| ì¬í˜„ìœ¨ (Recall)    | ì‹¤ì œ ì°¸ì¸ ê²ƒ ì¤‘ ì˜ˆì¸¡ë„ ì°¸ì¸ ë¹„ìœ¨   |
| F1-score           | ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™” í‰ê·         |

> ì˜ˆ: ìŠ¤íŒ¸ í•„í„°, ì•” ì§„ë‹¨ì—ì„œ ì¬í˜„ìœ¨ì´ ì¤‘ìš”
>  ë°˜ëŒ€ë¡œ ì‹ ìš©ì¹´ë“œ ì‚¬ê¸° íƒì§€ì—ì„œëŠ” ì •ë°€ë„ê°€ ì¤‘ìš”

**ğŸ“Š 3) confusion matrix (í˜¼ë™ í–‰ë ¬)**

```
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test, y_pred))
```

|           | ì‹¤ì œ ì–‘ì„± | ì‹¤ì œ ìŒì„± |
| --------- | --------- | --------- |
| ì˜ˆì¸¡ ì–‘ì„± | TP        | FP        |
| ì˜ˆì¸¡ ìŒì„± | FN        | TN        |

**ğŸ“ˆ 4) ROC-AUC (ì´ì§„ ë¶„ë¥˜ë§Œ)**

```
from sklearn.metrics import roc_auc_score
roc_auc_score(y_test, y_prob[:,1])
```

> 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ. 0.5ëŠ” ëœë¤ê³¼ ê°™ìŒ.

**ğŸ“‹ 5) ì „ì²´ ë¦¬í¬íŠ¸**

```
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))
```

#### ğŸ§ª ì˜ˆì‹œ ì½”ë“œ (RandomForestClassifier)

```
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

data = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(
    data.data, data.target, stratify=data.target, random_state=42)

model = RandomForestClassifier()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred))
```

### âœ… 2. íšŒê·€ ë¬¸ì œ: ì˜ˆì¸¡ ë° í‰ê°€

#### ğŸ“Œ ì˜ˆì¸¡

```
y_pred = model.predict(X_test)
```

#### ğŸ“Œ í‰ê°€ ì§€í‘œ

| ì§€í‘œ                      | ì„¤ëª…                     | í•¨ìˆ˜                  |
| ------------------------- | ------------------------ | --------------------- |
| MSE (Mean Squared Error)  | ì œê³± ì˜¤ì°¨ í‰ê·            | `mean_squared_error`  |
| RMSE (Root MSE)           | âˆšMSE, í•´ì„ ì‰¬ì›€          | `np.sqrt(mse)`        |
| MAE (Mean Absolute Error) | ì ˆëŒ“ê°’ ì˜¤ì°¨ í‰ê·          | `mean_absolute_error` |
| RÂ² (ê²°ì • ê³„ìˆ˜)            | ì˜ˆì¸¡ë ¥ì˜ ë¹„ìœ¨ (1ì´ ìµœê³ ) | `r2_score`            |

```
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
```

#### ğŸ§ª ì˜ˆì‹œ ì½”ë“œ (LinearRegression)

```
from sklearn.linear_model import LinearRegression
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

data = fetch_california_housing()
X_train, X_test, y_train, y_test = train_test_split(
    data.data, data.target, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))
print("RÂ²:", r2_score(y_test, y_pred))
```

### âœ… 3. êµì°¨ ê²€ì¦ìœ¼ë¡œ ëª¨ë¸ í‰ê°€ ì•ˆì •í™”

```
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')  # ë¶„ë¥˜
scores = cross_val_score(model, X, y, cv=5, scoring='r2')         # íšŒê·€
print("í‰ê·  ì„±ëŠ¥:", scores.mean())
```

### âœ… 4. í‰ê°€ ì§€í‘œ ì„ íƒ ìš”ë ¹

| ë¬¸ì œ ìœ í˜• | ìƒí™© ì˜ˆì‹œ | ì¶”ì²œ ì§€í‘œ              |
| --------- | --------- | ---------------------- |
| ì´ì§„ ë¶„ë¥˜ | ì•” ì§„ë‹¨   | `Recall`, `ROC-AUC`    |
| ì´ì§„ ë¶„ë¥˜ | ê¸ˆìœµ ì‚¬ê¸° | `Precision`, `F1`      |
| ë‹¤ì¤‘ ë¶„ë¥˜ | ë‰´ìŠ¤ ë¶„ë¥˜ | `Accuracy`, `macro F1` |
| íšŒê·€      | ì£¼ê°€ ì˜ˆì¸¡ | `RMSE`, `MAE`, `RÂ²`    |

### ğŸ” ì „ì²´ ìš”ì•½

| ë‹¨ê³„                      | ì„¤ëª…                     |
| ------------------------- | ------------------------ |
| `predict()`               | ì˜ˆì¸¡ê°’ ìƒì„±              |
| `accuracy_score()`        | ì •í™•ë„ í‰ê°€              |
| `confusion_matrix()`      | í˜¼ë™ í–‰ë ¬ ë¶„ì„           |
| `classification_report()` | ì£¼ìš” ë¶„ë¥˜ ì§€í‘œ ì¼ê´„ ì¶œë ¥ |
| `mean_squared_error()`    | íšŒê·€ ì˜¤ì°¨                |
| `cross_val_score()`       | êµì°¨ ê²€ì¦ ì •í™•ë„ í‰ê°€    |

## `TensorFlow`, `PyTorch` í”„ë ˆì„ì›Œí¬ ê°œìš”

| í•­ëª©                  | TensorFlow                               | PyTorch                                   |
| --------------------- | ---------------------------------------- | ----------------------------------------- |
| ğŸ’» ê°œë°œì‚¬              | Google                                   | Meta (Facebook)                           |
| ğŸ§  ì£¼ìš” ëª©ì            | ìƒìš© ë°°í¬, ëª¨ë°”ì¼, ë‹¤ì¤‘ í”Œë«í¼ ì§€ì›      | ì—°êµ¬/ê°œë°œ ìœ ì—°ì„±, ì‹¤í—˜ ë° í”„ë¡œí† íƒ€ì´í•‘    |
| ğŸ§® ê³„ì‚° ê·¸ë˜í”„         | ì •ì  (Static Graph)                      | ë™ì  (Dynamic Graph, define-by-run)       |
| ğŸ›  ëŒ€í‘œ API            | `tf.keras` (ê³ ìˆ˜ì¤€ API ì¤‘ì‹¬)             | `torch.nn`, `torch.optim` (ìœ ì—°í•œ ì €ìˆ˜ì¤€) |
| ğŸ§© ì‚¬ìš© ë‚œì´ë„         | ì´ˆì‹¬ìì—ê²Œ ì‰¬ì›€ (`fit`, `evaluate`)      | Pythonicí•˜ì§€ë§Œ ìˆ˜ë™ ë£¨í”„ í•„ìš”             |
| ğŸ” ë””ë²„ê¹…              | ì–´ë µê³  ë¹„ì§ê´€ì  (Graph ê¸°ë°˜)             | Pythonì²˜ëŸ¼ ì§ì ‘ ë””ë²„ê¹… ê°€ëŠ¥               |
| âš™ï¸ ì»¤ìŠ¤í„°ë§ˆì´ì§• ììœ ë„ | ì¤‘ê°„ (Kerasì—ì„  ì œí•œ)                    | ë§¤ìš° ë†’ìŒ (ë³µì¡í•œ ë¶„ê¸°ë¬¸, ì¬ê·€ ëª¨ë¸ ë“±)   |
| ğŸ“¦ ë°°í¬ ë° ì„œë¹™        | ë§¤ìš° ê°•ë ¥ (`TF Serving`, `TFLite`, `JS`) | TorchScript, ONNX ë“±ìœ¼ë¡œ ì œí•œì  ì§€ì›      |
| ğŸš€ GPU/TPU ì§€ì›        | GPU + TPU (ê³µì‹ ì§€ì›)                    | GPU (TPUëŠ” ê°„ì ‘ ì§€ì›)                     |
| ğŸ“Š ì‹œê°í™” ë„êµ¬         | TensorBoard                              | TensorBoard, wandb, Visdom                |
| ğŸ¤– ì „ì´ í•™ìŠµ           | `tf.keras.applications`                  | `torchvision.models`, HuggingFace ë“±      |

### âœ¨ ê³µí†µ ê¸°ëŠ¥

- í…ì„œ(Tensor) ê¸°ë°˜ ê³ ì°¨ì› ì—°ì‚°
- ìë™ ë¯¸ë¶„ (Autograd)
- ì‹ ê²½ë§ êµ¬ì„± (Sequential, Custom Module)
- GPU ê°€ì† ì§€ì›
- í•™ìŠµ ë£¨í”„ êµ¬ì„± (`fit()` ë˜ëŠ” ì»¤ìŠ¤í…€ ë£¨í”„)
- ì „ì´ í•™ìŠµ, ì‚¬ì „ í›ˆë ¨ ëª¨ë¸, ì„œë¹™ API ì¡´ì¬

### ğŸ” ì˜ˆì œ ë¹„êµ

#### âœ… TensorFlow (Keras ê¸°ë°˜)

```
import tensorflow as tf

model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1)
])
model.compile(optimizer='adam', loss='mse')
model.fit(X_train, y_train, epochs=10)
```

#### âœ… PyTorch (ì§ì ‘ ë£¨í”„)

```
import torch
import torch.nn as nn
import torch.optim as optim

class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(10, 1)

    def forward(self, x):
        return self.fc(x)

model = Net()
optimizer = optim.Adam(model.parameters())
criterion = nn.MSELoss()

for epoch in range(10):
    optimizer.zero_grad()
    output = model(X_train)
    loss = criterion(output, y_train)
    loss.backward()
    optimizer.step()
```

### âœ… ì–¸ì œ ë¬´ì—‡ì„ ì“¸ê¹Œ?

| ëª©ì               | ì¶”ì²œ í”„ë ˆì„ì›Œí¬         |
| ----------------- | ----------------------- |
| ì…ë¬¸ í•™ìŠµ         | TensorFlow (`tf.keras`) |
| ì—°êµ¬/ë…¼ë¬¸ ì‹¤í—˜    | PyTorch                 |
| ì‚°ì—… ë°°í¬         | TensorFlow              |
| ë³µì¡í•œ êµ¬ì¡° ëª¨ë¸  | PyTorch                 |
| Google Cloud ì—°ë™ | TensorFlow              |
| ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…   | PyTorch                 |

### ğŸ§  ê²°ë¡ 

- **TensorFlow**: ê¸°ì—… í™˜ê²½, ë°°í¬, ë†’ì€ ìƒì‚°ì„±ì„ ì›í•œë‹¤ë©´
- **PyTorch**: ììœ ë„, ë””ë²„ê¹… í¸ì˜ì„±, ì‹¤í—˜ ì¤‘ì‹¬ ê°œë°œì„ ì›í•œë‹¤ë©´

### ë‹¤ìŒ ì£¼ì œ

- `fit()` ì—†ì´ í•™ìŠµ ë£¨í”„ ì§ì ‘ êµ¬í˜„í•˜ê¸°
- CNN / RNN êµ¬ì¡° ì„¤ê³„
- GPU í•™ìŠµ ë°©ë²• (`.cuda()` / `tf.device`)
- Hugging Face ì‚¬ì „í•™ìŠµ ëª¨ë¸ ì—°ë™